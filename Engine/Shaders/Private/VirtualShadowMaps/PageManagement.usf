// Copyright Epic Games, Inc. All Rights Reserved.

/*=============================================================================
	PageManagement.usf: 
=============================================================================*/

#include "../Common.ush"
#include "../LightGridCommon.ush"
#include "../SceneTexturesCommon.ush"
#include "../DeferredShadingCommon.ush"
#include "../MortonCode.ush"
#include "../Nanite/NaniteDataDecode.ush"
#include "../HairStrands/HairStrandsVisibilityCommon.ush"
#include "PageAccessCommon.ush"
#include "PageCacheCommon.ush"
#include "ProjectionCommon.ush"

#ifndef VSM_GENERATE_STATS
	#define VSM_GENERATE_STATS 0
#endif // VSM_GENERATE_STATS

#if VSM_GENERATE_STATS
RWStructuredBuffer<uint> OutStatsBuffer;
#endif // VSM_GENERATE_STATS

// Flags generated by per-pixel pass to determine which pages are required to provide shadow for the visible geometry
RWStructuredBuffer<uint> OutPageRequestFlags;

Texture2D<UlongType> VisBuffer64;
StructuredBuffer<int> VirtualShadowMapIdRemap;
uint NumDirectionalLightSmInds;
uint bPostBasePass;
float LodFootprintScale;
float PageDilationBorderSize;

// HAIR_TODO: move all data into a HairStrands UB
Texture2D<uint4> HairCategorizationTexture;
uint bUseHairData;

// Convenience to group up a bunch of data computed for a screen pixel
struct FScreenPixelData
{
	int2 PixelPos;
	float2 ScreenUV;
	float4 ClipPosition;
	float DeviceZ;
	float SceneDepth;
	float3 ViewPosition;
	
	// Note: Actual world position (not pre-translated)
	float3 WorldPosition;

	// NOTE: This data is only valid when executing after the base pass (bPostBasePass is true)
	FGBufferData GBufferData;
};

float2 CalcScreenUV(uint2 PixelPos)
{
	return (float2(PixelPos.xy) + View.ViewRectMin.xy + 0.5f) * View.BufferSizeAndInvSize.zw;
}

// Load DeviceZ from depth buffer (and possibly combine with nanite depth)
float GetDeviceZFromPixel(uint2 PixelPos)
{
	// Merge Nanite depth with depth buffer if enabled (i.e. we're before the base pass)
	float DeviceZ = 0.0f;
#if LOAD_DEPTH_FROM_NANITE_BUFFER
	// Load Nanite Depth
	UlongType VisPixel = VisBuffer64[uint2(View.ViewRectMin.xy) + PixelPos];
	uint DepthInt = 0;
	uint VisibleClusterIndex = 0;
	uint TriIndex = 0;
	UnpackVisPixel(VisPixel, DepthInt, VisibleClusterIndex, TriIndex);

	if (VisibleClusterIndex != 0xFFFFFFFF)
	{
		DeviceZ = asfloat(DepthInt);
	}
#endif // LOAD_DEPTH_FROM_NANITE_BUFFER
	return max(DeviceZ, LookupDeviceZ(CalcScreenUV(PixelPos)));
}

// Use an explicit DeviceZ value for this pixel
FScreenPixelData GetScreenPixelData(uint2 PixelPos, float DeviceZ)
{
	FScreenPixelData Data;
	Data.PixelPos = PixelPos;
	Data.DeviceZ = DeviceZ;
	Data.ScreenUV = CalcScreenUV(PixelPos);
	Data.SceneDepth = ConvertFromDeviceZ(Data.DeviceZ);
	Data.ClipPosition = float4(((Data.ScreenUV.xy - View.ScreenPositionScaleBias.wz) / View.ScreenPositionScaleBias.xy), Data.DeviceZ, 1.0f);

	float4 ViewH = mul(Data.ClipPosition, View.ClipToView);
	Data.ViewPosition = ViewH.xyz / ViewH.w;

	float4 TranslatedWorldH = mul(Data.ClipPosition, View.ClipToTranslatedWorld);
	float3 TranslatedWorldPosition = TranslatedWorldH.xyz / TranslatedWorldH.w;
	// Subtract the view pre-translation to get back to world space
	Data.WorldPosition = TranslatedWorldPosition - View.PreViewTranslation;
	return Data;
}

uint GetMipLevel(int VirtualShadowMapId, float3 WorldPosition, float SceneDepth)
{
	float Footprint = float(VSM_VIRTUAL_MAX_RESOLUTION_XY);
	FVirtualShadowMapProjectionShaderData ProjectionData = GetVirtualShadowMapProjectionData(VirtualShadowMapId);

	{
		// Compute footprint by projecting the approximate size of a camera pixel at the given depth to shadow space
		// NOTE: This doesn't take the screen XY position/FOV into account, which may or may not be desirable.
		
		// TODO: Roll into a uniform
		float2 RadiusXY = 1.0f / (View.ViewSizeAndInvSize.xy * View.ViewToClip._m00_m11);
		float RadiusScreen = min(RadiusXY.x, RadiusXY.y);
		float DepthScale = SceneDepth * View.ViewToClip[2][3] + View.ViewToClip[3][3];

		float RadiusWorld = DepthScale * RadiusScreen;

		float3 TranslatedWorldPosition = WorldPosition + ProjectionData.ShadowPreViewTranslation;
		float4 ShadowUVz = mul(float4(TranslatedWorldPosition, 1.0f), ProjectionData.TranslatedWorldToShadowUVMatrix);

		float4 RadiusClipH = mul(float4(RadiusWorld, 0.0f, ShadowUVz.w, 1.0f), ProjectionData.ShadowViewToClipMatrix);
		float RadiusClip = abs(RadiusClipH.x / RadiusClipH.w);
		Footprint = RadiusClip * float(2 * VSM_VIRTUAL_MAX_RESOLUTION_XY);
	}

	return CalcMipLevelFromFootprint(LodFootprintScale * Footprint, VSM_MAX_MIP_LEVELS);
}

void MarkPage(uint VirtualShadowMapId, uint MipLevel, float3 WorldPosition, float2 PageDilationOffset)
{
	FVirtualShadowMapProjectionShaderData ProjectionData = GetVirtualShadowMapProjectionData(VirtualShadowMapId);
	float4 ShadowUVz = mul(float4(WorldPosition + ProjectionData.ShadowPreViewTranslation, 1.0f), ProjectionData.TranslatedWorldToShadowUVMatrix);
	ShadowUVz.xyz /= ShadowUVz.w;

	// Check overlap vs the shadow map space
	// NOTE: XY test not really needed anymore with the precise cone test in the caller, but we'll leave it for the moment
	bool bInClip = ShadowUVz.w > 0.0f && 
		all(ShadowUVz.xyz <= ShadowUVz.w &&
			ShadowUVz.xyz >= float3(-ShadowUVz.ww, 0.0f));
	if (!bInClip)
	{
		return;
	}

	float2 PageAddressFloat = ShadowUVz.xy * CalcLevelDimsPages(MipLevel);
	uint2 PageAddress = uint2(PageAddressFloat);
	uint PageOffset = CalcPageOffset(VirtualShadowMapId, MipLevel, PageAddress);
	OutPageRequestFlags[PageOffset] = 1;

	// PageDilationBorderSize == 0 implies PageDilationOffset.xy == 0
	if (PageDilationBorderSize > 0.0f)
	{
		uint2 PageAddress2 = uint2(PageAddressFloat + PageDilationOffset);
		uint PageOffset2 = CalcPageOffset(VirtualShadowMapId, MipLevel, PageAddress2);
		if (PageOffset2 != PageOffset)
		{
			OutPageRequestFlags[PageOffset2] = 1;
		}
		uint2 PageAddress3 = uint2(max(float2(0, 0), PageAddressFloat - PageDilationOffset));
		uint PageOffset3 = CalcPageOffset(VirtualShadowMapId, MipLevel, PageAddress3);
		if (PageOffset3 != PageOffset)
		{
			OutPageRequestFlags[PageOffset3] = 1;
		}
	}
}

[numthreads(VSM_DEFAULT_CS_GROUP_XY, VSM_DEFAULT_CS_GROUP_XY, 1)]
void GeneratePageFlagsFromPixels(uint3 GroupId : SV_GroupID, uint GroupIndex : SV_GroupIndex, uint3 DispatchThreadId : SV_DispatchThreadID)
{
	// Morton order within a group so page access/atomics are more coherent and wave-swizzled gradients are possible.
	uint2 PixelPos = VSM_DEFAULT_CS_GROUP_XY * GroupId.xy + MortonDecode(GroupIndex);

	if (any(PixelPos >= uint2(View.ViewSizeAndInvSize.xy)))
	{
		return;
	}
	
	float3 WorldPosition = 0;
	float  SceneDepth    = 0;
	bool   bIsValid		 = false;
	if (bUseHairData)
	{
		FCategorizationData CatData = DecodeCategorizationData(HairCategorizationTexture.Load(uint3(PixelPos, 0)));
		FScreenPixelData HairPixel = GetScreenPixelData(PixelPos, CatData.ClosestDepth);
		WorldPosition = HairPixel.WorldPosition;
		SceneDepth = HairPixel.SceneDepth;
		bIsValid = CatData.PixelCoverage > 0;
	}
	else
	{
		FScreenPixelData Pixel = GetScreenPixelData(PixelPos, GetDeviceZFromPixel(PixelPos));
		WorldPosition = Pixel.WorldPosition;
		SceneDepth = Pixel.SceneDepth;
		bIsValid = true;
		if (bPostBasePass != 0)
		{
			bIsValid = GetGBufferData(Pixel.ScreenUV).ShadingModelID != SHADINGMODELID_UNLIT;
		}
	}

	// Dither pattern for page dilation
	// We don't need to to check all 8 adjacent pages; as long as there's at least a single pixel near the edge
	// the adjacent one will get mapped. In practice only checking one diagonal seems to work fine and have minimal
	// overhead.
	float2 PageDilationOffset = PageDilationBorderSize * 
		float2(GroupIndex & 1 ? 1.0f : -1.0f,
			   GroupIndex & 2 ? 1.0f : -1.0f);
		
	// excluding unlit to avoid including processing sky dome
	if (!bIsValid)
	{
		return;
	}
	
	// Directional lights
	for (uint Index = 0; Index < NumDirectionalLightSmInds; ++Index)
	{
		// Mark one page per directional light. Lights can have varying numbers of clipmap levels allocated which
		// will all be contiguous in the array, so we hop between them using the counts.
		int ClipmapStartId = VirtualShadowMapIdRemap[Index];
		FVirtualShadowMapProjectionShaderData BaseProjectionData = GetVirtualShadowMapProjectionData(ClipmapStartId);

		// Possibly use a single shadow projection data for the whole clipmap with just scales/biases for
		// each level (to handle snapping). For now we maintain one per level.
		//int ClipmapLevel = CalcClipmapLevel(length(WorldPosition - ProjectionData.ClipmapWorldOrigin), ClipmapResolutionLodBias);
		const int ClipmapLevel = CalcClipmapLevel(BaseProjectionData, WorldPosition);
		int ClipmapIndex = max(0, ClipmapLevel - BaseProjectionData.ClipmapLevel);
		if (ClipmapIndex < BaseProjectionData.ClipmapLevelCount)
		{
			MarkPage(ClipmapStartId + ClipmapIndex, 0, WorldPosition, PageDilationOffset);
		}
	}

	// Local lights
	{
		uint EyeIndex = 0; // ??
		uint3 GridCoordinate = ComputeLightGridCellCoordinate(PixelPos, SceneDepth, EyeIndex);
		uint GridLinearIndex = ComputeLightGridCellIndex(GridCoordinate, EyeIndex);
		const FCulledLightsGridData CulledLightGridData = GetCulledLightsGrid(GridLinearIndex, EyeIndex);

		LOOP
		for (uint Index = 0; Index < CulledLightGridData.NumLocalLights; ++Index)
		{
			const FLocalLightData LightData = GetLocalLightData(CulledLightGridData.DataStartIndex + Index, EyeIndex);
			const uint LightGridLightIndex = ForwardLightData.CulledLightDataGrid[CulledLightGridData.DataStartIndex + Index];

			float3 ToLight = normalize(LightData.LightPositionAndInvRadius.xyz - WorldPosition);
			// Also do precise cone test, since froxels are pretty coarse at times.
			if (dot(ToLight, LightData.LightDirectionAndShadowMask.xyz) < LightData.SpotAnglesAndSourceRadiusPacked.x)
			{
				continue;
			}

			// The Virtual Shadow Remap stores directional lights first
			int VirtualShadowMapId = VirtualShadowMapIdRemap[NumDirectionalLightSmInds + LightGridLightIndex];
			if (VirtualShadowMapId != INDEX_NONE)
			{
				bool bSpotLight = LightData.SpotAnglesAndSourceRadiusPacked.x > -2.0f;
				if( !bSpotLight )
				{
					VirtualShadowMapId += GetCubeFace( -ToLight );
				}

				uint MipLevel = GetMipLevel(VirtualShadowMapId, WorldPosition, SceneDepth);
				MarkPage(VirtualShadowMapId, MipLevel, WorldPosition, PageDilationOffset);
			}
		}
	}
}

int ClipmapFirstCoarseLevel;

[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void MarkCoarsePages(uint3 Index : SV_DispatchThreadID)
{
	int VirtualShadowMapId = int(Index.x);
	if (VirtualShadowMapId >= int(VirtualShadowMap.NumShadowMaps))
	{
		return;
	}

	FVirtualShadowMapProjectionShaderData ProjectionData = GetVirtualShadowMapProjectionData(VirtualShadowMapId);

	if (ProjectionData.LightType == LIGHT_TYPE_DIRECTIONAL)
	{
		// Idea here is the clipmaps already cover supersets of lower levels
		// Thus to get coarser pages we can just mark the center page(s) offset by a level/LOD bias
		// The limit on how far dense data goes out from the camera then becomes the world space size of the marked page(s) on the coarses clipmap
		// We could of course mark a broader set of pages in the coarses clipmap level, but the effective radius
		// even from just marking a single one is usually already large enough for the systems that need this
		// data (volumetric fog, translucent light volume).
		
		if (ProjectionData.ClipmapLevel > ClipmapFirstCoarseLevel)
		{
			// TODO: Optimize this... can be boiled down to be just in terms of the snap offsets
			float4 ShadowUVz = mul(float4(ProjectionData.ClipmapWorldOrigin + ProjectionData.ShadowPreViewTranslation, 1.0f), ProjectionData.TranslatedWorldToShadowUVMatrix);
			float2 VirtualTexelAddressFloat = ShadowUVz.xy * float(CalcLevelDimsTexels(0));
			float2 PageAddressFloat = VirtualTexelAddressFloat * float(1.0f / VSM_PAGE_SIZE);
			// NOTE: Page addresses round down/truncate normally, so grab the surrounding 4
			int4 PageAddressLowHigh = int4(floor(PageAddressFloat - 0.5f), ceil(PageAddressFloat - 0.5f));

			OutPageRequestFlags[CalcPageOffset(VirtualShadowMapId, 0, PageAddressLowHigh.xy)] = 1;
			OutPageRequestFlags[CalcPageOffset(VirtualShadowMapId, 0, PageAddressLowHigh.xw)] = 1;
			OutPageRequestFlags[CalcPageOffset(VirtualShadowMapId, 0, PageAddressLowHigh.zy)] = 1;
			OutPageRequestFlags[CalcPageOffset(VirtualShadowMapId, 0, PageAddressLowHigh.zw)] = 1;
		}
	}
	else
	{
		// Mark last mip
		uint PageOffset = CalcPageOffset(VirtualShadowMapId, VSM_MAX_MIP_LEVELS - 1, uint2(0, 0));
		OutPageRequestFlags[PageOffset] = 1;
	}
}



RWStructuredBuffer<uint> OutHPageFlags;
RWStructuredBuffer<uint4> PageRectBoundsOut;

[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void InitPageRectBounds(uint3 Index : SV_DispatchThreadID)
{
	if (Index.x < VSM_MAX_MIP_LEVELS * VirtualShadowMap.NumShadowMaps)
	{
		PageRectBoundsOut[Index.x] = uint4(VSM_LEVEL0_DIM_PAGES_XY, VSM_LEVEL0_DIM_PAGES_XY, 0, 0);
	}
}

/**
 * One thread per page table flag entry, one vertical grid row per shadow map
 */
[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void GenerateHierarchicalPageFlags(uint2 ThreadId : SV_DispatchThreadID, uint2 GroupId : SV_GroupID)
{
	// early out any overflowing threads.
	if (ThreadId.x >= VirtualShadowMap.PageTableSize)
	{
		return;
	}

	uint PageTableEntryIndex = ThreadId.x;
	// Use the group ID to ensure the compiler knows it is scalar / uniform
	uint ShadowMapID = GroupId.y;
	uint GlobalPageTableEntryIndex = ShadowMapID * VirtualShadowMap.PageTableSize + PageTableEntryIndex;
	// If the flag is set, let's get on with it

	uint Flag = PageFlags[GlobalPageTableEntryIndex];
	if (Flag)
	{
		// 

		// find the mip-level for this thread.
		uint MipLevel = 0;
		UNROLL
		for (MipLevel = 0; MipLevel < VSM_MAX_MIP_LEVELS - 1; ++MipLevel)
		{
			if (PageTableEntryIndex < CalcLevelOffsets( MipLevel + 1))
			{
				break;
			}
		}
		uint Level0RowMask = ((1U << VSM_LOG2_LEVEL0_DIM_PAGES_XY) - 1U);
		// Offset within level
		uint PageTableLevelOffset = PageTableEntryIndex - CalcLevelOffsets(MipLevel);
		// Coordinate within Mip Level
		uint PX = PageTableLevelOffset & (Level0RowMask >> MipLevel);
		uint PY = PageTableLevelOffset >> (VSM_LOG2_LEVEL0_DIM_PAGES_XY - MipLevel);

		// Compute the min/max rect of active pages
		uint PageBoundIndex = ShadowMapID * VSM_MAX_MIP_LEVELS + MipLevel;
		InterlockedMin(PageRectBoundsOut[PageBoundIndex].x, PX);
		InterlockedMin(PageRectBoundsOut[PageBoundIndex].y, PY);
		InterlockedMax(PageRectBoundsOut[PageBoundIndex].z, PX);
		InterlockedMax(PageRectBoundsOut[PageBoundIndex].w, PY);

		// Loop over H flag levels, this builds a mip pyramid over _each_ mip level in the page table
		// the 0-th level in this hiearchy is the page table mip level itself.
		uint MaxHLevel = VSM_MAX_MIP_LEVELS - MipLevel;
		uint HLevelSizePages = VSM_LEVEL0_DIM_PAGES_XY >> MipLevel;
		// Note: starting from 1 as level 0 is the ordinary flag mip level
		for (uint HMipLevel = 1U; HMipLevel < MaxHLevel; ++HMipLevel)
		{
			HLevelSizePages >>= 1U;
			PX >>= 1U;
			PY >>= 1U;
			uint HPageFlagOffset = ShadowMapID * VirtualShadowMap.HPageTableSize
				+ CalcHPageFlagLevelOffsets(MipLevel)
				+ CalcLevelOffsets(HMipLevel + MipLevel) 
				- CalcLevelOffsets(MipLevel + 1U) 
				+ PY * HLevelSizePages + PX;

			uint PreviousValue = 0;
			InterlockedOr(OutHPageFlags[HPageFlagOffset], Flag, PreviousValue);

			// If this was already the value, then whoever did that will continue up the hierarhcy, best of luck!
			if (PreviousValue == Flag)
			{
				break;
			}
		}
	}
}


// Counter used to allocate physical pages from a linear range, which is mapped to a Physical 2D texture (by wrapping)
RWStructuredBuffer<uint> AllocatedPagesOffset;
// Page flags generated by page allocation to indicate state to rendering passes (i.e., present / invalid)
RWStructuredBuffer<uint> OutPageFlags;
RWStructuredBuffer<uint> OutPageTable;

// Stores a uint2 with the physical location of cached pages in the previous frame physical texture, newly allocated ones are set to invalid index
RWStructuredBuffer<FCachedPageInfo> OutCachedPageInfos;

RWStructuredBuffer<uint> CoverageSummaryInOut;
StructuredBuffer<uint> PageRequestFlags;

void CreatePageMapping( uint ShadowMapID, uint MipLevel, uint2 PageAddress, uint PageOffset, bool bPageRequestFlag )
{	
	if( bPageRequestFlag )
	{
		uint pPageIndex = 0;
		InterlockedAdd( AllocatedPagesOffset[0], 1, pPageIndex );

#if VSM_GENERATE_STATS
		InterlockedAdd(OutStatsBuffer[0], 1);
#endif // VSM_GENERATE_STATS

		if (pPageIndex >= VirtualShadowMap.MaxPhysicalPages)
		{
			// We end up here if we're out of physical pages, this means some parts get no physical backing provided.
			// Post this error condition back to the host somehow!
			// Probably want to know if we're getting close even.
			OutPageTable[PageOffset] = VSM_PHYSICAL_PAGE_INVALID;
			OutPageFlags[PageOffset] = 0;
			return;
		}

		// Wrap linear page index into physical 2D space
		uint2 pPage;
		pPage.x = pPageIndex  & VirtualShadowMap.PhysicalPageRowMask;
		pPage.y = pPageIndex >> VirtualShadowMap.PhysicalPageRowShift;

		OutPageTable[PageOffset] = ShadowEncodePageTable(pPage);

		FCachedPageInfo CachedPageInfo;

		CachedPageInfo.PhysPageAddress = uint2(VSM_INVALID_PHYSICAL_PAGE_ADDRESS, VSM_INVALID_PHYSICAL_PAGE_ADDRESS);
		CachedPageInfo.DepthOffset = 0.0f;
		CachedPageInfo.Padding = 0.0f;

#if HAS_CACHE_DATA
		if (ShadowMapCacheData[ShadowMapID].VirtualShadowMapId != INDEX_NONE)
		{
			CachedPageInfo.DepthOffset = ShadowMapCacheData[ShadowMapID].DepthOffset;

			int2 PrevPageAddress = int2(PageAddress) - (ShadowMapCacheData[ShadowMapID].SmPageOffset >> MipLevel);
			// A page cannot be cached if it is above the alignment level, or the light has not moved
			// This means that for small movement a CSM can cache everything as well.
			bool bCanCache = MipLevel < VSM_CACHE_ALIGNMENT_LEVEL || all(ShadowMapCacheData[ShadowMapID].SmPageOffset == 0);
			if (all(uint2(PrevPageAddress) < CalcLevelDimsPages(MipLevel)) && bCanCache)
			{
				uint PrevOffset = CalcPageOffset(ShadowMapCacheData[ShadowMapID].VirtualShadowMapId, MipLevel, PrevPageAddress);
				uint PrevPageFlag = PrevPageFlags[PrevOffset];
				// True if the page had moving stuff drawn into it
				bool PrevDynPageFlag = PrevDynamicCasterPageFlags[PrevOffset] != 0;

#if VSM_GENERATE_STATS
				// stats for those pages who die from moving stuff
				if (PrevPageFlag && PrevDynPageFlag)
				{
					InterlockedAdd(OutStatsBuffer[2], 1);
				}
#endif // VSM_GENERATE_STATS
				// If it was either rendered or cached previous frame, and not drawn to by a moving thing we may re-use
				if (PrevPageFlag && !PrevDynPageFlag)
				{
					uint2 PrevPhysAddress = ShadowDecodePageTable(PrevPageTable[PrevOffset]).PageIndex;

					// Only re-use this page if it was actually rendered to (i.e., mapped during in a render call)
					FPhysicalPageMetaData MetaData = PrevPhysicalPageMetaData[PhysPageAddressToIndex(PrevPhysAddress)];
					if (MetaData.State == VSM_PHYSICAL_PAGE_STATE_RENDERED)
					{
// 'rand robin' invalidation
#if 0
						// If cached for too long invalidate
						if (MetaData.Age > 3 + (PageOffset & 3))
						{
#if VSM_GENERATE_STATS
							InterlockedAdd(OutStatsBuffer[4], 1);
#endif // VSM_GENERATE_STATS
						}
						else
#endif
						{
#if VSM_GENERATE_STATS
							InterlockedAdd(OutStatsBuffer[1], 1);
#endif // VSM_GENERATE_STATS

							CachedPageInfo.PhysPageAddress = PrevPhysAddress;
						}
					}
				}
// Needs more work (must disable page AND ensure fallback page is enabled)!
#if 0
				else
				{
					// Find fallback
					uint PrevIncCoarserPageFlag = PrevPageFlag;
					uint PrevCoarseOffset = 0;
					// Check coarser levels (up to 4 levels away, arbitrary, hardcoded)
					for (uint CoarseMipLevel = MipLevel + 1; CoarseMipLevel < min(MipLevel + 4, VSM_MAX_MIP_LEVELS); ++CoarseMipLevel)
					{
						PrevCoarseOffset = CalcPageOffset(ShadowMapCacheData[ShadowMapID].VirtualShadowMapId, CoarseMipLevel, PrevPageAddress >> (CoarseMipLevel - MipLevel));
						uint PrevCoarserPageFlag = PrevPageFlags[PrevCoarseOffset];
						if (PrevCoarserPageFlag)
						{
							PrevIncCoarserPageFlag = 1;
							break;
						}
					}
					if (PrevIncCoarserPageFlag)
					{
#if VSM_GENERATE_STATS
#endif // VSM_GENERATE_STATS
						InterlockedAdd(OutStatsBuffer[2], 1);
						CachedPageInfo.PhysPageAddress = ShadowDecodePageTable(PrevPageTable[PrevCoarseOffset]).PageIndex;
					}
				}
#endif
			}
		}
#endif // HAS_CACHE_DATA

		// Store data that is used later in the process.
		OutCachedPageInfos[pPageIndex] = CachedPageInfo;
		// Request rendering of pages that are not cached (AKA VSM_INVALID_PHYSICAL_PAGE_ADDRESS in cache).
		uint Flags = (CachedPageInfo.PhysPageAddress.x == VSM_INVALID_PHYSICAL_PAGE_ADDRESS ? VSM_INVALID_FLAG : 0) | VSM_ALLOCATED_FLAG;
		OutPageFlags[PageOffset] = Flags;
	}
	else
	{
		OutPageTable[PageOffset] = VSM_PHYSICAL_PAGE_INVALID;
		OutPageFlags[PageOffset] = 0;
	}
}

/**
 * One thread per 2x2 page flags in level 0 (aka num in level 1), launched as 1d groups, with 2D grid with Y dim ==  NumShadowMaps.
 */
[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void CreatePageMappings(uint2 ThreadId : SV_DispatchThreadID, uint2 GroupId : SV_GroupID)
{
	const uint NumLevel1Entries = VSM_LEVEL0_DIM_PAGES_XY * VSM_LEVEL0_DIM_PAGES_XY / 4;

	// early out any overflowing threads.
	if (ThreadId.x >= NumLevel1Entries)
	{
		return;
	}

#if VSM_GENERATE_STATS
	if (all(ThreadId == 0))
	{
		OutStatsBuffer[3] = VirtualShadowMap.NumShadowMaps;
	}
#endif // VSM_GENERATE_STATS

	uint Level0RowMask = ((1U << VSM_LOG2_LEVEL0_DIM_PAGES_XY) - 1U);

	// Use the group ID to ensure the compiler knows it is scalar / uniform
	uint ShadowMapID = GroupId.y;
	// Get 1D offset in level1
	uint PageTableEntryIndex = ThreadId.x;

	uint2 vPage;
	vPage.x = PageTableEntryIndex & (Level0RowMask >> 1);
	vPage.y = PageTableEntryIndex >> (VSM_LOG2_LEVEL0_DIM_PAGES_XY - 1);

	uint ChildCoverage = 0;

	// Init mip0 pages
	for( uint Child = 0; Child < 4; Child++ )
	{
		uint2 vChildPage = vPage * 2 + uint2( Child & 1, Child >> 1 );

		uint PageOffset = CalcPageOffset( ShadowMapID, 0, vChildPage );
		uint Flag = PageRequestFlags[ PageOffset ];

		CreatePageMapping( ShadowMapID, 0, vChildPage, PageOffset, Flag != 0 );

		ChildCoverage |= Flag << Child;
	}

	uint AllCovered = ChildCoverage;

	for (uint Level = 1; Level < VSM_MAX_MIP_LEVELS; ++Level)
	{
		uint PageOffset = CalcPageOffset( ShadowMapID, Level, vPage );
		uint Flag = PageRequestFlags[ PageOffset ];

		bool bIsAllCovered = AllCovered == 0xf;	//0b1111

		// Don't kill the last mip. It may be forced on.
		if( Level == VSM_MAX_MIP_LEVELS - 1 )
		{
			bIsAllCovered = false;
		}
		
		CreatePageMapping( ShadowMapID, Level, vPage, PageOffset, Flag && !bIsAllCovered );
		
		if( Level == VSM_MAX_MIP_LEVELS - 1 )
			break;
		
		uint2 Child = vPage & 1;
		uint ChildIndex = Child.x + Child.y * 2;
		AllCovered = (Flag || bIsAllCovered) ? (1 << ChildIndex) : 0;
		
		uint PackedCoverage;
		PackedCoverage  = AllCovered;
		PackedCoverage |= 1 << 16;

		vPage >>= 1;
		uint NextOffset = CalcPageOffset( ShadowMapID, Level + 1, vPage );
		
		// Add coverage and retrieve sum + counter
		// Propagate atomically to the next level, to only let last thread to go on we store 1 in upper bits
		uint OtherPackedCoverage = 0;
		InterlockedAdd( CoverageSummaryInOut[ NextOffset ], PackedCoverage, OtherPackedCoverage );

		// 3 out of 4 threads will exit here, as they are done (moving up mip chain they're not needed), only last one will continue.
		// NOTE: this leads to poor utilization but is usually far preferable to multiple passes, also whole SIMDs exit fairly quickly. 
		//       One could probably schedule the threads a bit more intelligently...
		if (OtherPackedCoverage < (3 << 16))
		{
			break;
		}

		// Add the coverage of the first three in 2x2 (returned by atomic).
		PackedCoverage += OtherPackedCoverage;
		
		AllCovered = ( PackedCoverage ) & 0xf;
	}
}

/**
* One thread per page in level 0, launched as 1d groups, with 2D grid with Y dim ==  NumShadowMaps.
* This is effectively just a big broadcast operation. There are more efficient ways to do this with
* fewer threads and wave ops, but given the page counts just relying on memory coalescing is
* good enough for now.
*/
[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void PropagateMappedMips(uint2 ThreadId : SV_DispatchThreadID, uint2 GroupId : SV_GroupID)
{
	const uint NumLevel0Entries = VSM_LEVEL0_DIM_PAGES_XY * VSM_LEVEL0_DIM_PAGES_XY;
	if (ThreadId.x >= NumLevel0Entries)
	{
		return;
	}

	int VirtualShadowMapID = int(GroupId.y);
	uint PageTableEntryIndex = ThreadId.x;

	uint2 Level0Page;
	Level0Page.x = PageTableEntryIndex & ((1U << VSM_LOG2_LEVEL0_DIM_PAGES_XY) - 1U);
	Level0Page.y = PageTableEntryIndex >> VSM_LOG2_LEVEL0_DIM_PAGES_XY;

	FVirtualShadowMapProjectionShaderData ProjectionData = GetVirtualShadowMapProjectionData(VirtualShadowMapID);

	if (ProjectionData.LightType == LIGHT_TYPE_DIRECTIONAL)
	{
		// Directional lights propagate pages to their coarser/larger clipmap levels (and only use mip0 pages)
		// Each clipmap level is a separate VSM, so we gather any mapped coarser pages as necessary and write only our own page output
		// There's also technically a race similar to below with other threads writing the PT data we are reading,
		// but it's still deterministic as long as we only look at pages with "bThisLODValid".
		// There's some redundant work of course, but this shader is pretty cheap overall

		uint Page0Offset = CalcPageOffset(VirtualShadowMapID, 0, Level0Page);
		FShadowPhysicalPage pPage0 = ShadowDecodePageTable(OutPageTable[Page0Offset]);

		BRANCH
		if (!pPage0.bThisLODValid)
		{
			const int OffsetScale = (VSM_LEVEL0_DIM_PAGES_XY >> 2);
			int2 BaseOffset = OffsetScale * ProjectionData.ClipmapCornerOffset;
			int2 BasePage   = int2(Level0Page) - BaseOffset;

			// Search for first mapped page past this one
			uint RemainingLevels = ProjectionData.ClipmapLevelCount - ProjectionData.ClipmapIndex;
			for (uint ClipmapOffset = 1; ClipmapOffset < RemainingLevels; ++ClipmapOffset)
			{
				const int ClipmapLevelId = VirtualShadowMapID + int(ClipmapOffset);

				FVirtualShadowMapProjectionShaderData LevelProjectionData = GetVirtualShadowMapProjectionData(ClipmapLevelId);								
				int2 LevelOffset = OffsetScale * LevelProjectionData.ClipmapCornerOffset;

				const int LevelScale = 1 << ClipmapOffset;
				int2 LevelPage = (BasePage + LevelScale * LevelOffset) / LevelScale;

				if (all(LevelPage >= 0) && all(LevelPage < VSM_LEVEL0_DIM_PAGES_XY))
				{
					uint LevelPageOffset = CalcPageOffset(ClipmapLevelId, 0, uint2(LevelPage));
					FShadowPhysicalPage pPage = ShadowDecodePageTable(OutPageTable[LevelPageOffset]);
					if (pPage.bThisLODValid)
					{
						OutPageTable[Page0Offset] = ShadowEncodePageTable(pPage.PageIndex, ClipmapOffset);
						break;
					}
				}
				else
				{
					// TODO: We're off the edge... can this ever even happen in practice given the construction?
				}
			}
		}
	}
	else
	{
		// Local lights propagate pages to their coarser mips
		int MappedPageLevel = -1;
		uint2 MappedPageIndex = 0;
	
		for (int Level = (VSM_MAX_MIP_LEVELS - 1); Level >= 0; --Level)
		{
			uint2 vPage = Level0Page >> Level;
			uint PageOffset = CalcPageOffset(VirtualShadowMapID, Level, vPage);
			FShadowPhysicalPage pPage = ShadowDecodePageTable(OutPageTable[PageOffset]);

			BRANCH
			if (pPage.bThisLODValid)
			{
				// This page is mapped, so leave it alone and propagate downwards
				MappedPageLevel = Level;
				MappedPageIndex = pPage.PageIndex;
			}
			else if( MappedPageLevel >= 0 )
			{
				// This page is not mapped; replace it with our suitably offset parent mapped page
				// Ensure only one thread writes each value to avoid races, but we read on all threads as the broadcast
				// Note that this can race with the other threads reading this value, but since bThisLODValid will
				// always be false on these updated pages the values will be ignored. As long as the writes to the page
				// table are atomic (currently a single DWORD), this is safe.
				if (all((vPage << Level) == Level0Page))
				{
					uint MipOffset = MappedPageLevel - Level;
					OutPageTable[PageOffset] = ShadowEncodePageTable(MappedPageIndex, MipOffset);
				}
			}
		}
	}
}





#define LOG2_TILE_SIZE	5
#define LOG2_TILES_PER_PAGE_X ( VSM_LOG2_PAGE_SIZE - LOG2_TILE_SIZE )
#define LOG2_TILES_PER_PAGE_XY ( 2 * LOG2_TILES_PER_PAGE_X )

#define TILES_PER_PAGE_X_MASK ( ( 1 << LOG2_TILES_PER_PAGE_X ) - 1 )
#define TILES_PER_PAGE_XY_MASK ( ( 1 << LOG2_TILES_PER_PAGE_XY ) -1 )

StructuredBuffer<uint> NumAllocatedPhysicalPages;

RWBuffer<uint> ClearPhysicalPagesArgs;
RWTexture2D< uint > PhysicalPagesTexture;

StructuredBuffer<FCachedPageInfo> CachedPageInfos;
Texture2D< uint > CachedPhysicalPagesTexture;

RWStructuredBuffer<FPhysicalPageMetaData> PhysicalPageMetaDataOut;

[numthreads( 1, 1, 1 )]
void InitClearPhysicalPagesArgs(uint3 DTID : SV_DispatchThreadID)
{
	uint PageCount = NumAllocatedPhysicalPages[ 0 ];
	ClearPhysicalPagesArgs[ 0 ] = (PageCount << uint(LOG2_TILES_PER_PAGE_XY));
	ClearPhysicalPagesArgs[ 1 ] = 1;
	ClearPhysicalPagesArgs[ 2 ] = 1;
}

[numthreads( 16, 16, 1 )]
void ClearPhysicalPages( uint3 TileThreadID : SV_GroupThreadID, uint3 TileIndex : SV_GroupID )
{
	const uint PageIndex = TileIndex.x >> uint(LOG2_TILES_PER_PAGE_XY);
	const uint LocalTileIndex = TileIndex.x & uint(TILES_PER_PAGE_XY_MASK);

	const uint LocalTileX = LocalTileIndex & uint(TILES_PER_PAGE_X_MASK);
	const uint LocalTileY = LocalTileIndex >> uint(LOG2_TILES_PER_PAGE_X);

	uint ClearValue[4] = { 0u, 0u, 0u, 0u };

	FPhysicalPageMetaData MetaData;
	MetaData.State = VSM_PHYSICAL_PAGE_STATE_CLEARED;
	MetaData.Age = 0;

#if HAS_CACHE_DATA
	// Load data from cache if available
	FCachedPageInfo CachedPageInfo = CachedPageInfos[PageIndex];
	if (CachedPageInfo.PhysPageAddress.x != VSM_INVALID_PHYSICAL_PAGE_ADDRESS)
	{
		const uint2 TileOffset = (CachedPageInfo.PhysPageAddress << uint2(VSM_LOG2_PAGE_SIZE, VSM_LOG2_PAGE_SIZE)) + (uint2(LocalTileX, LocalTileY) << uint2(LOG2_TILE_SIZE, LOG2_TILE_SIZE));
		const uint2 BasePos = TileOffset + (TileThreadID.xy << 1u);
		ClearValue[0] = asuint(max(0.0f, asfloat(CachedPhysicalPagesTexture[BasePos + uint2(0, 0)]) + CachedPageInfo.DepthOffset));
		ClearValue[1] = asuint(max(0.0f, asfloat(CachedPhysicalPagesTexture[BasePos + uint2(1, 0)]) + CachedPageInfo.DepthOffset));
		ClearValue[2] = asuint(max(0.0f, asfloat(CachedPhysicalPagesTexture[BasePos + uint2(0, 1)]) + CachedPageInfo.DepthOffset));
		ClearValue[3] = asuint(max(0.0f, asfloat(CachedPhysicalPagesTexture[BasePos + uint2(1, 1)]) + CachedPageInfo.DepthOffset));

		MetaData = PrevPhysicalPageMetaData[PhysPageAddressToIndex(CachedPageInfo.PhysPageAddress)];
		// This data has survived one more frame
		MetaData.Age += 1;
	}
#endif // HAS_CACHE_DATA

	const uint PageX = PageIndex & VirtualShadowMap.PhysicalPageRowMask;
	const uint PageY = PageIndex >> VirtualShadowMap.PhysicalPageRowShift;
	const uint2 TileOffset = (uint2(PageX, PageY) << uint2(VSM_LOG2_PAGE_SIZE, VSM_LOG2_PAGE_SIZE)) + (uint2(LocalTileX, LocalTileY) << uint2(LOG2_TILE_SIZE, LOG2_TILE_SIZE));

	// Update physical page meta data
	if (LocalTileIndex == 0u && all(TileThreadID.xy == 0u))
	{
		PhysicalPageMetaDataOut[PageIndex] = MetaData;
	}

	const uint2 BasePos = TileOffset + (TileThreadID.xy << uint2(1u, 1u));
	PhysicalPagesTexture[BasePos + uint2(0u, 0u)] = ClearValue[0];
	PhysicalPagesTexture[BasePos + uint2(1u, 0u)] = ClearValue[1];
	PhysicalPagesTexture[BasePos + uint2(0u, 1u)] = ClearValue[2];
	PhysicalPagesTexture[BasePos + uint2(1u, 1u)] = ClearValue[3];
}


/**
 * Invoke to update the sate of all pages after copying & clearing has been performed.
 */
[numthreads(64, 1, 1)]
void UpdatePhysicalPageMetaDataClearState(uint PageIndex : SV_DispatchThreadID)
{
	uint PageCount = NumAllocatedPhysicalPages[0];
	if (PageIndex < PageCount)
	{
		FPhysicalPageMetaData MetaData;
		MetaData.State = VSM_PHYSICAL_PAGE_STATE_CLEARED;
		MetaData.Age = 0;

#if HAS_CACHE_DATA
		// Load data from cache if available
		FCachedPageInfo CachedPageInfo = CachedPageInfos[PageIndex];
		if (CachedPageInfo.PhysPageAddress.x != VSM_INVALID_PHYSICAL_PAGE_ADDRESS)
		{
			MetaData = PrevPhysicalPageMetaData[PhysPageAddressToIndex(CachedPageInfo.PhysPageAddress)];
			// This data has survived one more frame
			MetaData.Age += 1;
		}
#endif // HAS_CACHE_DATA

		PhysicalPageMetaDataOut[PageIndex] = MetaData;
	}
}



Texture2D< float > PrevPhysicalPagePoolHw;


uint bRectPrimitive;
RWBuffer<uint> ClearPhysicalPageDrawArgs;

[numthreads(1, 1, 1)]
void SetupClearHwPhysicalPagesDrawArgs(uint3 DTID : SV_DispatchThreadID)
{
	uint PageCount = NumAllocatedPhysicalPages[0];
	// Set up indirect draw args for likewise clearing the HW physical page args
	ClearPhysicalPageDrawArgs[0] = bRectPrimitive > 0 ? 4 : 6;
	ClearPhysicalPageDrawArgs[1] = PageCount;
	ClearPhysicalPageDrawArgs[2] = 0;
	ClearPhysicalPageDrawArgs[3] = 0;
}


void InitializePhysicalPagesVS(
	in uint InVertexId : SV_VertexID,
	in uint InInstanceId : SV_InstanceID,
	nointerpolation out uint PageIndex : TEXCOORD0,
	out float4 OutPosition : SV_POSITION)
{
	PageIndex = InInstanceId;
	const uint PageX = PageIndex & VirtualShadowMap.PhysicalPageRowMask;
	const uint PageY = PageIndex >> VirtualShadowMap.PhysicalPageRowShift;

	uint2 TileVertex = uint2(PageX, PageY) * VSM_PAGE_SIZE;
	TileVertex.x += InVertexId == 1 || InVertexId == 2 || InVertexId == 4 ? VSM_PAGE_SIZE : 0;
	TileVertex.y += InVertexId == 2 || InVertexId == 4 || InVertexId == 5 ? VSM_PAGE_SIZE : 0;

	OutPosition = float4(float2(TileVertex) * VirtualShadowMap.RecPhysicalPoolSize.xy * float2(2.0f, -2.0f) + float2(-1.0, 1.0f), 0.5f, 1.0f);
}

void InitializePhysicalPagesPS(uint PageIndex : TEXCOORD0, float4 SVPos : SV_POSITION, out float OutDepth : SV_Depth)
{
	const uint PageX = PageIndex & VirtualShadowMap.PhysicalPageRowMask;
	const uint PageY = PageIndex >> VirtualShadowMap.PhysicalPageRowShift;

	// figure out the relative position within the page
	uint2 OutPixelPos = uint2(SVPos.xy);
	// Position within page
	uint2 PagePixelPos = OutPixelPos - uint2(PageX, PageY) * VSM_PAGE_SIZE;
	float ClearValue = 0.0f;

#if HAS_CACHE_DATA
	// Load data from cache if available
	FCachedPageInfo CachedPageInfo = CachedPageInfos[PageIndex];
	if (CachedPageInfo.PhysPageAddress.x != VSM_INVALID_PHYSICAL_PAGE_ADDRESS)
	{
		const uint2 CachedPos = CachedPageInfo.PhysPageAddress * VSM_PAGE_SIZE + PagePixelPos;
		ClearValue = max(0.0f, PrevPhysicalPagePoolHw.Load(int3(CachedPos, 0)) + CachedPageInfo.DepthOffset);
	}
#endif // HAS_CACHE_DATA

	OutDepth = ClearValue;
}

RWStructuredBuffer<FPhysicalPageMetaData> InOutPhysicalPageMetaData;
StructuredBuffer<uint> VirtualShadowMapFlags;


/**
 * Processes all page tables organized as a linear block, page table entries for mip levels are in a consecutive range,
 * so it makes no difference to this process which shadow map or level is being handled.
 * Run with Dispatch size: {NumVirtualShadowMaps * VirtualShadowMap.PageTableSize / VSM_DEFAULT_CS_GROUP_X, 1, 1}
 */
[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void MarkRenderedPhysicalPages(uint3 PageTableEntryIndex : SV_DispatchThreadID)
{
	// Because of launch size rounding we might get here.
	if (PageTableEntryIndex.x >= VirtualShadowMap.NumShadowMaps * VirtualShadowMap.PageTableSize)
	{
		return;
	}

	uint ShadowMapID = PageTableEntryIndex.x / VirtualShadowMap.PageTableSize;

	// This is estupido, should at least use array of IDs to skip all unused ones...
	if (VirtualShadowMapFlags[ShadowMapID] == 0)
	{
		return;
	}

	FShadowPhysicalPage pPage = ShadowGetPhysicalPage(PageTableEntryIndex.x);
	if (pPage.bThisLODValid)
	{
		uint PageIndex = PhysPageAddressToIndex(pPage.PageIndex);
		FPhysicalPageMetaData MetaData = InOutPhysicalPageMetaData[PageIndex];
		if (MetaData.State != VSM_PHYSICAL_PAGE_STATE_INVALID)
		{
			MetaData.State = VSM_PHYSICAL_PAGE_STATE_RENDERED;
			// NOTE: error condition, add ensure when we have a sensible compiler for GPU...
		}
		InOutPhysicalPageMetaData[PageIndex] = MetaData;
	}
}


/**
 */
[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void InitPhysicalPageMetaData(uint3 Index : SV_DispatchThreadID)
{
	FPhysicalPageMetaData MetaData;
	MetaData.State = VSM_PHYSICAL_PAGE_STATE_INVALID;
	MetaData.Age = 0;

	// Because of launch size rounding we might get here.
	if (Index.x < VirtualShadowMap.MaxPhysicalPages)
	{
		PhysicalPageMetaDataOut[Index.x] = MetaData;
	}
}


StructuredBuffer<uint> InputCountBuffer;
RWBuffer<uint> IndirectDispatchArgsOut;
uint Multiplier;
uint Divisor;
uint InputCountOffset;

[numthreads( 1, 1, 1 )]
void InitIndirectArgs1D(
	uint3 DTID : SV_DispatchThreadID)
{
	uint GroupCount = (InputCountBuffer[InputCountOffset] * Multiplier + Divisor - 1U) / Divisor;

	IndirectDispatchArgsOut[ 0 ] = GroupCount;
	IndirectDispatchArgsOut[ 1 ] = 1;
	IndirectDispatchArgsOut[ 2 ] = 1;
}




// 1. Calc page rects. Dispacth size {VSM_LEVEL0_DIM_PAGES_XY,VSM_LEVEL0_DIM_PAGES_XY,NumSms}
[numthreads(VSM_DEFAULT_CS_GROUP_XY, VSM_DEFAULT_CS_GROUP_XY, 1)]
void CalculatePageRects(uint3 DispatchThreadID : SV_DispatchThreadID)
{
	// One group per each mip 0
	uint ShadowMapID = DispatchThreadID.z;
	uint2 vPage = DispatchThreadID.xy;

	UNROLL
	for (uint MipLevel = 0; MipLevel < VSM_MAX_MIP_LEVELS; ++MipLevel)
	{
		// Early out for out of bounds 
		if (any(vPage >= uint2(VSM_LEVEL0_DIM_PAGES_XY >> MipLevel, VSM_LEVEL0_DIM_PAGES_XY >> MipLevel)))
		{
			return;
		}

		uint PageOffset = CalcPageOffset(ShadowMapID, MipLevel, vPage);
		uint Flag = PageFlags[PageOffset];
		if (Flag)
		{
			// Compute the min/max rect of requested pages
			uint PageBoundIndex = ShadowMapID * VSM_MAX_MIP_LEVELS + MipLevel;
			InterlockedMin(PageRectBoundsOut[PageBoundIndex].x, vPage.x);
			InterlockedMin(PageRectBoundsOut[PageBoundIndex].y, vPage.y);
			InterlockedMax(PageRectBoundsOut[PageBoundIndex].z, vPage.x);
			InterlockedMax(PageRectBoundsOut[PageBoundIndex].w, vPage.y);
		}
		OutPageTable[PageOffset] = VSM_PHYSICAL_PAGE_INVALID;

	}
}


bool TestRectOverlap(uint4 A, uint4 B)
{
	return all(A.xy <= B.zw) && all(A.zw >= B.xy);
}

#define MAX_PAGE_RECTS_BUFFER 1024

// Store compacted range of index+area to skip all empty ones at least
groupshared uint2 RectsToProcess[MAX_PAGE_RECTS_BUFFER];
groupshared uint NumRectsToProcessGroupShared;

groupshared uint MaxAreaAndIndex;
groupshared uint NextFreeOffsetX;

groupshared uint4 AllocatedRectsGroupShared[MAX_PAGE_RECTS_BUFFER];
groupshared uint NumAllocatedRectsGroupShared;

RWStructuredBuffer<uint4> AllocatedPageRectBoundsOut;

// 2. Sort page rects (just find max)
// 3. Allocate page rects in phys space. (serial)
[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void AllocatePagesUsingRects(uint GroupThreadId : SV_GroupThreadID, uint2 DispatchThreadID : SV_DispatchThreadID)
{
	if (GroupThreadId == 0)
	{
		NumRectsToProcessGroupShared = 0;
		MaxAreaAndIndex = 0;
		NextFreeOffsetX = 0;
		NumAllocatedRectsGroupShared = 0;
	}
	GroupMemoryBarrierWithGroupSync();

	uint TotalMips = VirtualShadowMap.NumShadowMaps * VSM_MAX_MIP_LEVELS;
	// Load all rects that are non-empty
	for (uint Index = GroupThreadId; Index < TotalMips; Index += VSM_DEFAULT_CS_GROUP_X)
	{
		uint4 Rect = PageRectBounds[Index];
		AllocatedPageRectBoundsOut[Index] = uint4(VSM_LEVEL0_DIM_PAGES_XY, VSM_LEVEL0_DIM_PAGES_XY, 0, 0);
		if (all(Rect.zw >= Rect.xy))
		{
			uint WriteOffset = 0;
			uint Area = (Rect.z - Rect.x + 1) * (Rect.w - Rect.y + 1);
			InterlockedAdd(NumRectsToProcessGroupShared, 1, WriteOffset);
			if (WriteOffset < MAX_PAGE_RECTS_BUFFER)
			{
				RectsToProcess[WriteOffset] = uint2(Index, Area);
			}
		}
	}
	GroupMemoryBarrierWithGroupSync();
	// Protect against overflow (but should also detect and somehow fix)
	uint NumRectsToProcess = min(MAX_PAGE_RECTS_BUFFER, NumRectsToProcessGroupShared);
	// Loop until all the rects are processed, each iteration finds the max, and then processes that.
	// Note: this is extremely stupid! If we had a simple to use sorting function for group shared then we could make it less extra stupid.
	for (uint IterIndex = 0; IterIndex < NumRectsToProcess; ++IterIndex)
	{
		// Group stride loop to find max index using atomics
		for (uint Index = GroupThreadId; Index < NumRectsToProcess; Index += VSM_DEFAULT_CS_GROUP_X)
		{
			uint2 Item = RectsToProcess[Index];
			// Pack area and item index (not source rect index)
			uint AreaAndIndex = (Item.y << 12U) | Index;
			InterlockedMax(MaxAreaAndIndex, AreaAndIndex);
		}
		GroupMemoryBarrierWithGroupSync();

		// Find item that won the bidding
		uint ItemIndex = MaxAreaAndIndex & ((1U << 12U) - 1U);
		// Get info
		uint2 Item = RectsToProcess[ItemIndex];
		// Reset stuff and clear the winning rect so it won't win again
		GroupMemoryBarrierWithGroupSync();
		if (GroupThreadId == 0)
		{
			RectsToProcess[ItemIndex] = uint2(0U, 0U);
			MaxAreaAndIndex = 0U;
		}
		GroupMemoryBarrierWithGroupSync();
		// Now we have the next largest item, must find allocation

		// Allocation Algo
		// Scan each possible position across each row, 
		// test overlap against all existing allocations and skip horizontally
		// when we run out of row, move to next and repeat
		uint4 Rect = PageRectBounds[Item.x];
		uint ShadowMapID = Item.x / VSM_MAX_MIP_LEVELS;
		uint MipLevel = Item.x % VSM_MAX_MIP_LEVELS;
		uint2 RectSize = Rect.zw - Rect.xy;
		// Scan each row where it could fit
		for (uint RowInd = 0; RowInd < VirtualShadowMap.PhysicalPoolSizePages.y - RectSize.y; ++RowInd)
		{
			// Current probe position, test against all rects using group stride loop
			uint2 TestPos = uint2(0U, RowInd);
			uint4 ProbeRect = uint4(TestPos.xy, TestPos.xy + RectSize.xy);
			bool bFoundFree = false;
			
			// Until we run off the edge 
			while (ProbeRect.z < VirtualShadowMap.PhysicalPoolSizePages.x)
			{
				NextFreeOffsetX = 0;
				GroupMemoryBarrierWithGroupSync();
				for (uint Index = GroupThreadId; Index < NumAllocatedRectsGroupShared; Index += VSM_DEFAULT_CS_GROUP_X)
				{
					uint4 AllocatedRect = AllocatedRectsGroupShared[Index];
					if (TestRectOverlap(AllocatedRect, ProbeRect))
					{
						// Find free location to the right of all rects on this row
						InterlockedMax(NextFreeOffsetX, AllocatedRect.z + 1);
					}
				}
				GroupMemoryBarrierWithGroupSync();
				// If we did not overlap anything that modified the rect, then we found a free space.
				if (NextFreeOffsetX == 0U)
				{
					bFoundFree = true;
					break;
				}
				else
				{
					// Bump probe rect for next iteration
					ProbeRect.xz = uint2(NextFreeOffsetX, NextFreeOffsetX + RectSize.x);
				}
				// need to sync before next iteration also or else we may read the NextFreeOffsetX out of order
				GroupMemoryBarrierWithGroupSync();
			}

			if (bFoundFree)
			{
				if (GroupThreadId == 0)
				{
					AllocatedRectsGroupShared[NumAllocatedRectsGroupShared] = ProbeRect;
					NumAllocatedRectsGroupShared += 1;
					AllocatedPageRectBoundsOut[Item.x] = ProbeRect;
				}
					// 4. Allocate pages to rects (1:1)
				for (uint PY = 0U; PY <= RectSize.y; ++PY)
				{
					// Group stride loop in X
					for (uint PX = GroupThreadId; PX <= RectSize.x; PX += VSM_DEFAULT_CS_GROUP_X)
					{
						// Virtual page
						uint2 vPage = Rect.xy + uint2(PX, PY);
						uint2 pPage = ProbeRect.xy + uint2(PX, PY);

						FCachedPageInfo CachedPageInfo;

						CachedPageInfo.PhysPageAddress = uint2(VSM_INVALID_PHYSICAL_PAGE_ADDRESS, VSM_INVALID_PHYSICAL_PAGE_ADDRESS);
						CachedPageInfo.DepthOffset = 0.0f;
						CachedPageInfo.Padding = 0.0f;
						// Store data that is used later in the process (for each physical page).
						OutCachedPageInfos[pPage.y * VirtualShadowMap.PhysicalPoolSizePages.x + pPage.x] = CachedPageInfo;

						uint PageOffset = CalcPageOffset(ShadowMapID, MipLevel, vPage);
						// Request rendering of pages that are not cached (AKA VSM_INVALID_PHYSICAL_PAGE_ADDRESS in cache).
						// TODO: make sure caching machinery is use in this path also
						OutPageFlags[PageOffset] = VSM_INVALID_FLAG | VSM_ALLOCATED_FLAG;
						OutPageTable[PageOffset] = ShadowEncodePageTable(pPage);
					}
				}

				break;
			}
		}
		GroupMemoryBarrierWithGroupSync();
	}
}
