// Copyright 1998-2019 Epic Games, Inc. All Rights Reserved.

/*=============================================================================
NiagaraSimulationShader.usf: 
=============================================================================*/
#pragma warning(disable:4008)
#include "/Engine/Public/Platform.ush"

#if GPU_SIMULATION
	#include "/Engine/Private/Common.ush"
	#include "/Engine/Private/GlobalDistanceFieldShared.ush"
#else
	const static float PI = 3.1415926535897932f;
#endif
	const static float TWO_PI = 3.1415926535897932f*2.0f;

#include "/Engine/Private/Definitions.usf"

// Most of the vector implementations work this way. This helps us keep proper precision.
float4 ModuloPrecise(float4 x, float4 y){ return x - y * trunc(x/y); }
float3 ModuloPrecise(float3 x, float3 y){ return x - y * trunc(x/y); }
float2 ModuloPrecise(float2 x, float2 y){ return x - y * trunc(x/y);}
float ModuloPrecise(float x, float y){ return x - y * trunc(x/y); }
int ModuloPrecise(int x, int y){ return x - y * (x/y); }
int Modulo(int x, int y){  return x - y * (x/y);  }

	
// using rcp is only 12 bits of precision, we should usually pay for perf
float4 Reciprocal(float4 x){ return 1.0f/x;}
float3 Reciprocal(float3 x){ return 1.0f/x; }
float2 Reciprocal(float2 x){ return 1.0f/x;}
float Reciprocal(float x){ return 1.0f/x; }

/* -----------------------------------------------------------------
 * GPU simulation utility functions
 * -----------------------------------------------------------------
 */
#if GPU_SIMULATION

	static uint3 GDispatchThreadId;
	static uint3 GGroupThreadId;
	static uint GCurrentPhase;
	static uint GEmitterTickCounter;
	static uint GSimStart;
	static uint GRandomSeedOffset = 0;

	// To be removed as well but don't know who is using it
	#include "/Engine/Private/SceneTexturesCommon.ush"

	// Temporary include while everything is moved to plugins
	#if USE_SHADER_STAGES == 1
		#include "NiagaraStrandsPhysics.ush"
	#endif

	uint EmitterTickCounter;

	

	float4 Modulo(float4 x, float4 y){ return fmod(x,y); }
	float3 Modulo(float3 x, float3 y){ return fmod(x,y); }
	float2 Modulo(float2 x, float2 y){ return fmod(x,y); }
	float Modulo(float x, float y){ return fmod(x,y); }

	// utility function used for scene depth calculations
	float3 WorldPositionFromSceneDepth(float2 ScreenPosition, float SceneDepth)
	{
		float4 HomogeneousWorldPosition = mul(float4(ScreenPosition * SceneDepth, SceneDepth, 1), View.ScreenToWorld);
		return HomogeneousWorldPosition.xyz / HomogeneousWorldPosition.w;
	}

	// MASSIVE HACK - Tracked in JIRA UE-69298
	// Hardcoded random function accessible from inner part of node implementation.
	// It works for now at least and avoids exposing every random needed in the UI. 
	// Temporary solution, it will be replaced when a design is validated.
	float NiagaraInternalNoise(uint u, uint v, uint s)
	{
		uint Seed = (u * 1664525u + v) + s + GRandomSeedOffset;
		GRandomSeedOffset += Seed;
		return float(Rand3DPCG32(int3(u,v,Seed)).x) / 4294967296.0f;
	}

	// NIAGARA_MAX_GPU_SPAWN_INFOS is set from the shader compiler
	#define NIAGARA_MAX_GPU_SPAWN_INFOS_V4	((NIAGARA_MAX_GPU_SPAWN_INFOS + 3) / 4)

	float4	EmitterSpawnInfoOffsets[NIAGARA_MAX_GPU_SPAWN_INFOS_V4];
	float4	EmitterSpawnInfoParams[NIAGARA_MAX_GPU_SPAWN_INFOS];			// Packed data where x = IntervalDt, y = InterpStartDt, z = Group, w = Start Particle Index

	static int GInterpSpawnIndex;
	static float Emitter_SpawnInterval;
	static float Emitter_InterpSpawnStartDt;
	static int Emitter_SpawnGroup;

	void GetEmitterSpawnInfoForParticle(int ParticleIndex)
	{
		int SpawnInfoIndex = 0;
		UNROLL
		for ( int i=0; i < NIAGARA_MAX_GPU_SPAWN_INFOS_V4; ++i )
		{
			SpawnInfoIndex += (int)dot(1.0f, ParticleIndex >= EmitterSpawnInfoOffsets[i]);
		}

		Emitter_SpawnInterval		= EmitterSpawnInfoParams[SpawnInfoIndex].x;
		Emitter_InterpSpawnStartDt	= EmitterSpawnInfoParams[SpawnInfoIndex].y;
		Emitter_SpawnGroup			= asint(EmitterSpawnInfoParams[SpawnInfoIndex].z);

		GInterpSpawnIndex		= ParticleIndex - asint(EmitterSpawnInfoParams[SpawnInfoIndex].w);
	}
#endif

/* ----------------------------------------------------------------------------
 * Seeded/Deterministic random number generation functions
 *
 * This is a variant of NiagaraRand4DPCG32 from Random.ush. 
 *
 * uint is not fully supported in the VM so we simply use ints and drop the 
 * top and bottom bit swap. This should be fine since signed overflow should 
 * produce the same results as unsigned overflow when comparing bit-by-bit on 
 * all relevant architectures.
 *
 * Warning: Only contains 24 bits of randomness, since we produce values in 
 *          the unit interval. Uses the upper 24 bits, as they have the best
 *          quality.
 *
 *          By removing the bit swaps in NiagaraRand4DPCG32 we save a few 
 *          operations, but lose a bit of statistical (but not visual) quality, 
 *          and for our use case this is an acceptable compromise.
 * ----------------------------------------------------------------------------
 */

// Returns 4 random normalized floats based on 4 explicit integer seeds
float4 rand4(int Seed1, int Seed2, int Seed3, int Seed4)
{
	int4 v = int4(Seed4, Seed1, Seed2, Seed3) * 1664525 + 1013904223;

	v.x += v.y*v.w;
	v.y += v.z*v.x;
	v.z += v.x*v.y;
	v.w += v.y*v.z;
	v.x += v.y*v.w;
	v.y += v.z*v.x;
	v.z += v.x*v.y;
	v.w += v.y*v.z;

	// We can use 24 bits of randomness, as all integers in [0, 2^24] 
	// are exactly representable in single precision floats.
	// We use the upper 24 bits as they tend to be higher quality.

	// The divide is often folded with the range scale in the rand functions
	return float4((v >> 8) & 0x00ffffff) / 16777216.0; // 0x01000000 == 16777216
	// return float4((v >> 8) & 0x00ffffff) * (1.0/16777216.0); // bugged, see UE-67738
}

// float3 specialization of the above:
// 
// Returns 3 random normalized floats based on 4 explicit integer seeds.
// 
// All bits of the first and second seeds are used, while only 
// the lower 16 bits of the third and fourth seeds are used.
float3 rand3(int Seed1, int Seed2, int Seed3, int Seed4)
{
	int3 v = int3(Seed1, Seed2, Seed4 | (Seed3 << 16)) * 1664525 + 1013904223;

	v.x += v.y*v.z;
	v.y += v.z*v.x;
	v.z += v.x*v.y;
	v.x += v.y*v.z;
	v.y += v.z*v.x;
	v.z += v.x*v.y;

	return float3((v >> 8) & 0x00ffffff) / 16777216.0; // 0x01000000 == 16777216
}

// Internal counter used to generate a different sequence of random numbers for each call
static int RandomCounterDeterministic = 0;

// Cost using rand4: 6 imad, 1 itof, 1 ishr, 1 add, 2 mul
float rand(float x, int Seed1, int Seed2, int Seed3)
{
	RandomCounterDeterministic += 1;
	return rand3(Seed1, Seed2, Seed3, RandomCounterDeterministic).x * x;
}

// Cost using rand4: 7 imad, 1 itof, 1 ishr, 1 add, 2 mul
float2 rand(float2 x, int Seed1, int Seed2, int Seed3)
{
	RandomCounterDeterministic += 1;
	return rand3(Seed1, Seed2, Seed3, RandomCounterDeterministic).xy * x;
}

// Cost using rand4: 8 imad, 1 itof, 1 ishr, 1 add, 2 mul
float3 rand(float3 x, int Seed1, int Seed2, int Seed3)
{
	RandomCounterDeterministic += 1;
	return rand3(Seed1, Seed2, Seed3, RandomCounterDeterministic).xyz * x;
}

// Cost using rand4: 9 imad, 1 itof, 1 ishr, 1 and, 2 mul
float4 rand(float4 x, int Seed1, int Seed2, int Seed3) 
{
	RandomCounterDeterministic += 1;
	return rand4(Seed1, Seed2, Seed3, RandomCounterDeterministic).xyzw * x;
}

// Cost using rand4: 6 imad, 2 itof, 1 ishr, 1 add, 2 mul, 1 ftoi
int rand(int x, int Seed1, int Seed2, int Seed3)
{
	// Scaling a uniform float range provides better distribution of numbers than using %.
	// Inclusive! So [0, x] instead of [0, x)
	RandomCounterDeterministic += 1;
	return int(rand3(Seed1, Seed2, Seed3, RandomCounterDeterministic).x * (x+1));
}

/* -----------------------------------------------------------------
 * Un-seeded/Non-deterministic random number generation functions
 * -----------------------------------------------------------------
 */
#if GPU_SIMULATION
	// This simply calls the deterministic random number functions from the Seeded RNG section, 
	// but uses non-deterministic seeds as input. 

	// This could perhaps be optimized by using slightly cheaper functions, but the difference is likely negligible. 
	
	// Internal counter used to generate a different sequence of random numbers for each call
	// We need to keep this separate from the Deterministic version so that non-deterministic 
	// calls do not interfere with the deterministic ones. 
	static int RandomCounterNonDeterministic = -1;

	float rand(float x)
	{
		RandomCounterNonDeterministic -= 1;
		return rand4(GDispatchThreadId.x, EmitterTickCounter, GDispatchThreadId.x, RandomCounterNonDeterministic).x * x;
	}

	float2 rand(float2 x)
	{
		RandomCounterNonDeterministic -= 1;
		return rand4(GDispatchThreadId.x, EmitterTickCounter, GDispatchThreadId.x, RandomCounterNonDeterministic).xy * x;
	}

	float3 rand(float3 x)
	{
		RandomCounterNonDeterministic -= 1;
		return rand4(GDispatchThreadId.x, EmitterTickCounter, GDispatchThreadId.x, RandomCounterNonDeterministic).xyz * x;
	}

	float4 rand(float4 x) 
	{
		RandomCounterNonDeterministic -= 1;
		return rand4(GDispatchThreadId.x, EmitterTickCounter, GDispatchThreadId.x, RandomCounterNonDeterministic).xyzw * x;
	}

	// Integer randoms are INCLUSIVE, i.e. includes both the upper and lower limits
	int rand(int x)
	{
		RandomCounterNonDeterministic -= 1;
		return int(rand4(GDispatchThreadId.x, EmitterTickCounter, GDispatchThreadId.x, RandomCounterNonDeterministic).x * (x+1));
	}
#else
	// Old unseeded, passthrough to FRandomStream
	float2 rand(float2 x)
	{
		return float2(rand(x.x), rand(x.y));
	}

	float3 rand(float3 x)
	{
		return float3(rand(x.x), rand(x.y), rand(x.z));
	}

	float4 rand(float4 x) 
	{
		return float4(rand(x.x), rand(x.y), rand(x.z), rand(x.w));
	}
	
	// NOTE(mv): Where's rand(float)? Seems to work regardless..
	int rand(int x);
#endif


/* -----------------------------------------------------------------
 * VM simulation function declarations
 * -----------------------------------------------------------------
 */
#if VM_SIMULATION
	float noise(float x);
	float noise(float2 x);
	float noise(float3 x);

	//Total hack to get around the cross compiler converting fmod() to "X - (Y * trunc(X/Y))";
	//On gpu just define these as fmod(x,y)
	float4 Modulo(float4 x, float4 y);
	float3 Modulo(float3 x, float3 y);
	float2 Modulo(float2 x, float2 y);
	float Modulo(float x, float y);

	/** Returns the index for this particle in the current execution context. On gpu this'll likely be derived from DispatchThreadId */
	int ExecIndex();

	//Some functions that we use to map to special VM operations for reading in data.
	//TODO: replace with proper buffer reading capability and use standard hlsl.
	int AcquireIndex(int DataSetID, bool DoAcquire);

	void AcquireID(int DataSetID, out int IDIndex, out int IDAcquireTag);
	void UpdateID(int DataSetID, int IDIndex, int InstanceIndex);

	float InputDataFloat(int DataSetIndex, int RegisterIdx);  //DataSetIndex is 0 for main dataset
	int InputDataInt(int DataSetIndex, int RegisterIdx);
	bool InputDataBool(int DataSetIndex, int RegisterIdx);

	float InputDataNoadvanceFloat(int DataSetIndex, int RegisterIdx);  //DataSetIndex is 0 for main dataset
	int InputDataNoadvanceInt(int DataSetIndex, int RegisterIdx);
	bool InputDataNoadvanceBool(int DataSetIndex, int RegisterIdx);

	void OutputDataFloat(int DataSetIndex, int RegisterIndex, int InstanceIndex, float Value);
	void OutputDataInt(int DataSetIndex, int RegisterIndex, int InstanceIndex, int Value);
	void OutputDataBool(int DataSetIndex, int RegisterIndex, int InstanceIndex, bool Value);
#endif




/* -----------------------------------------------------------------
 * GPU simulation code
 * -----------------------------------------------------------------
 */
#if GPU_SIMULATION
	static uint GSpawnStartInstance;
		
	/* Parameters for spawning and simulations
	 */
	uint UpdateStartInstance;
	
	/*
	 * declared as int instead of uint to avoid compiler warnings because we can't expose uint on Niagara module parameters.  
	 * This is still declared as a uint in the corresponding C++ classes
	 */
	int DefaultShaderStageIndex;
	int ShaderStageIndex;


	int IterationInterfaceCount;
	uint SpawnedInstances;

	uint ComponentBufferSizeRead;
	uint ComponentBufferSizeWrite;
	int	NumEventsPerParticle;
	int NumParticlesPerEvent;
	uint CopyInstancesBeforeStart;

	uint SimStart;

	/* Buffers for particle data and DrawIndirect calls
	 */
	Buffer<float> InputFloat;
	Buffer<int> InputInt;
	RWBuffer<int> RWOutputInt;
	RWBuffer<float> RWOutputFloat;

	RWBuffer<uint> RWInstanceCounts;
	uint ReadInstanceCountOffset;
	uint WriteInstanceCountOffset;

	//TODO: IDs for GPU
//	void AcquireID(int DataSetID, out int IDIndex, out int IDAcquireTag);
// 	void UpdateID(int DataSetID, int IDIndex, int InstanceIndex);

	/* Returns the current instance index relative to the operation (spawn/update)
	 */
	int ExecIndex()
	{
		if (GCurrentPhase == 0)
		{
			return UpdateStartInstance + GDispatchThreadId.x - GSpawnStartInstance;
		}
		else 
		{
			return UpdateStartInstance + GDispatchThreadId.x;
		}
	}

#define USE_WAVE_INTRISICS (PS4_PROFILE || XBOXONE_PROFILE)
#define USE_GROUP_SHARED ((THREADGROUP_SIZE == 64 || THREADGROUP_SIZE == 32)  && !USE_WAVE_INTRISICS)

#if USE_GROUP_SHARED
	#if THREADGROUP_SIZE == 64
		groupshared uint GroupSharedIndex[64];
		groupshared uint GroupSharedIndex4[16];
		groupshared uint GroupSharedIndex16[4];
		groupshared uint GroupSharedIndex64;
	#elif THREADGROUP_SIZE == 32
		groupshared uint GroupSharedIndex[32];
		groupshared uint GroupSharedIndex4[8];
		groupshared uint GroupSharedIndex16[2];
		groupshared uint GroupSharedIndex64;
	#endif
#endif // USE_GROUP_SHARED

	/* Acquire an output index - the default index is the scratch instance; one additional instance is allocated 
	 *	at the end of the buffer, so no branching on -1 is necessary during OutputData operations
	 */
	int AcquireIndex(uniform int DataSetID, bool bDoAcquire)
	{
		// Begin static assert : GPU particles only support DataSetID 0
		int MustBe0[1];
		MustBe0[DataSetID] = 0;
		// End static assert

		#if USE_SHADER_STAGES
			if(ShaderStageIndex != 0)
			{
				bDoAcquire = false; // Prevent any change to the atomic
			}
		#endif // USE_SHADER_STAGES

		int PrevIdx = GSpawnStartInstance + SpawnedInstances;	// scratch instance as default; write to that for dead particles

		#if USE_WAVE_INTRISICS

			uint NumCounters = WaveActiveCountBits(bDoAcquire);
			uint PrefixCounts = WavePrefixCountBits(bDoAcquire);
			if(NumCounters)
			{
				if(WaveIsFirstLane())
				{
					uint RetPrevIdx;
					InterlockedAdd(RWInstanceCounts[WriteInstanceCountOffset], NumCounters, RetPrevIdx);
					PrevIdx = (int)RetPrevIdx;
				}

				if(bDoAcquire)
				{
					PrevIdx = WaveReadFirstLane(PrevIdx);
					PrevIdx += PrefixCounts;
				}
			}	

		#elif USE_GROUP_SHARED

			GroupSharedIndex[GGroupThreadId.x] = bDoAcquire ? 1 : 0;
			GroupMemoryBarrierWithGroupSync(); 

			// Group by 4
			if ((GGroupThreadId.x & 0x3) == 0)
			{
				const uint Index = GGroupThreadId.x;

				const uint ActiveCount1 = GroupSharedIndex[Index];
				const uint ActiveCount2 = ActiveCount1 + GroupSharedIndex[Index + 1];
				const uint ActiveCount3 = ActiveCount2 + GroupSharedIndex[Index + 2];
				const uint ActiveCount4 = ActiveCount3 + GroupSharedIndex[Index + 3];
				
				GroupSharedIndex[Index] = 0;
				GroupSharedIndex[Index + 1] = ActiveCount1;
				GroupSharedIndex[Index + 2] = ActiveCount2;
				GroupSharedIndex[Index + 3] = ActiveCount3;
				GroupSharedIndex4[Index / 4] = ActiveCount4;
			}
			GroupMemoryBarrierWithGroupSync(); 

			// Group by 16
			if ((GGroupThreadId.x & 0xF) == 0)
			{
				const uint Index = GGroupThreadId.x / 4;

				const uint ActiveCount1 = GroupSharedIndex4[Index];
				const uint ActiveCount2 = ActiveCount1 + GroupSharedIndex4[Index + 1];
				const uint ActiveCount3 = ActiveCount2 + GroupSharedIndex4[Index + 2];
				const uint ActiveCount4 = ActiveCount3 + GroupSharedIndex4[Index + 3];

				GroupSharedIndex4[Index] = 0;
				GroupSharedIndex4[Index + 1] = ActiveCount1;
				GroupSharedIndex4[Index + 2] = ActiveCount2;
				GroupSharedIndex4[Index + 3] = ActiveCount3;
				GroupSharedIndex16[Index / 4] = ActiveCount4;
			}
			GroupMemoryBarrierWithGroupSync(); 

			// Group by 64
			if (GGroupThreadId.x == 0)
			{
				const uint ActiveCount1 = GroupSharedIndex16[0];
				const uint ActiveCount2 = ActiveCount1 + GroupSharedIndex16[1];
		#if THREADGROUP_SIZE == 64
				const uint ActiveCount3 = ActiveCount2 + GroupSharedIndex16[2];
				const uint ActiveCount4 = ActiveCount3 + GroupSharedIndex16[3];
		#endif
				GroupSharedIndex16[0] = 0;
				GroupSharedIndex16[1] = ActiveCount1;
		#if THREADGROUP_SIZE == 64
				GroupSharedIndex16[2] = ActiveCount2;
				GroupSharedIndex16[3] = ActiveCount3;
		#endif
				uint RetPrevIdx = 0;
		#if THREADGROUP_SIZE == 64
				InterlockedAdd(RWInstanceCounts[WriteInstanceCountOffset], ActiveCount4, RetPrevIdx);
		#elif THREADGROUP_SIZE == 32
				InterlockedAdd(RWInstanceCounts[WriteInstanceCountOffset], ActiveCount2, RetPrevIdx);
		#endif
				GroupSharedIndex64 = RetPrevIdx;
			}
			GroupMemoryBarrierWithGroupSync(); 

			PrevIdx = GroupSharedIndex64 + GroupSharedIndex16[GGroupThreadId.x / 16] + GroupSharedIndex4[GGroupThreadId.x / 4] + GroupSharedIndex[GGroupThreadId.x];

		#else // !USE_WAVE_INTRISICS && !USE_GROUP_SHARED

			if(bDoAcquire == true)
			{
				// Have to use uint's here to avoid PS4 compiler warnings about InterlockedAdd, cannot propagate uint due to CPU VM limitations...
				uint RetPrevIdx;
				// @TODO : add some TLS logic to avoid thread group for doing atomic for each thread. (gathering the actual required count)
				InterlockedAdd(RWInstanceCounts[WriteInstanceCountOffset], (uint)1U, RetPrevIdx);
				PrevIdx = (int)RetPrevIdx;
			}

		#endif // USE_WAVE_INTRISICS || USE_GROUP_SHARED

		#if USE_SHADER_STAGES
			if(ShaderStageIndex != 0)
			{
				return ExecIndex();
			}
		#endif // USE_SHADER_STAGES

		return PrevIdx;
	}

	/* ---------------------------------------------------------------------
	 * InputData operations 
	 * ---------------------------------------------------------------------
	 */
	float InputDataFloat(int DataSetIndex, int RegisterIdx, int InstanceIdx)
	{
		return InputFloat[RegisterIdx*ComponentBufferSizeRead + InstanceIdx];
	}

	int InputDataInt(int DataSetIndex, int RegisterIdx, int InstanceIdx)
	{
		return InputInt[RegisterIdx*ComponentBufferSizeRead + InstanceIdx];
	}
	
	bool InputDataBool(int DataSetIndex, int RegisterIdx, int InstanceIdx)
	{
		return InputInt[RegisterIdx*ComponentBufferSizeRead + InstanceIdx] == -1;
	}


	/* ---------------------------------------------------------------------
	 * OutputData operations 
	 * ---------------------------------------------------------------------
	 */
	void OutputDataFloat(int DataSetIndex, int RegisterIndex, int InstanceIndex, float Value)
	{
		RWOutputFloat[RegisterIndex*ComponentBufferSizeWrite + InstanceIndex + UpdateStartInstance] = Value;
	}

	void OutputDataInt(int DataSetIndex, int RegisterIndex, int InstanceIndex, int Value)
	{
		RWOutputInt[RegisterIndex*ComponentBufferSizeWrite + InstanceIndex + UpdateStartInstance] = Value;
	}

	void OutputDataBool(int DataSetIndex, int RegisterIndex, int InstanceIndex, bool Value)
	{
		RWOutputInt[RegisterIndex*ComponentBufferSizeWrite + InstanceIndex + UpdateStartInstance] = Value ? -1 : 0;
	}

	void EnterStatScope(int ID)	{}
	void ExitStatScope()	{}
#endif // GPU_SIMULATION

	/* 
	 * Get the index to write onto the output buffer
	 */
	int OutputIndex(const int DataSetID, const bool bIsPersistent, const bool bIsValid)
	{
		#if USE_SHADER_STAGES
			const int DenseIndex = AcquireIndex(DataSetID,bIsValid);
			const int SparseIndex = GDispatchThreadId.x;
			return (bIsPersistent && (DenseIndex != SparseIndex) ) ? SparseIndex : DenseIndex;
		#else
			return AcquireIndex(DataSetID,bIsValid);
		#endif // USE_SHADER_STAGES
	}

//Include the simulation shader code generated by the node graph.
#include "/Engine/Generated/NiagaraEmitterInstance.ush"


#if GPU_SIMULATION

/* 
 *	CS wrapper for our generated code; calls spawn and update functions on the corresponding instances in the buffer
 */

[numthreads(THREADGROUP_SIZE, 1, 1)]
void SimulateMainComputeCS(
	uint3 GroupId : SV_GroupID,
	uint3 DispatchThreadId : SV_DispatchThreadID,
	uint3 GroupThreadId : SV_GroupThreadID)
{
	GDispatchThreadId = DispatchThreadId;
	GGroupThreadId = GroupThreadId;
	GCurrentPhase = -1;
	GEmitterTickCounter = EmitterTickCounter;
	GSimStart = SimStart;
	GRandomSeedOffset = 0;

	/*
	if(CopyInstancesBeforeStart == 1)
	{
		UpdateStartInstance = 0;
	}
	*/

	const uint InstanceID = UpdateStartInstance + DispatchThreadId.x;
	if (ReadInstanceCountOffset == 0xFFFFFFFF)
	{
		GSpawnStartInstance = 0;
	}
	else
	{
		GSpawnStartInstance = RWInstanceCounts[ReadInstanceCountOffset];				// needed by ExecIndex()
	}
	const int MaxInstances = GSpawnStartInstance + SpawnedInstances;

	bool bRunPhase1 = InstanceID < GSpawnStartInstance && InstanceID < UpdateStartInstance + MaxInstances;
	bool bRunPhase0 = InstanceID >= GSpawnStartInstance && InstanceID < UpdateStartInstance + MaxInstances;
	
	#if USE_SHADER_STAGES
	if (IterationInterfaceCount > 0)
	{
		bRunPhase1 = InstanceID < IterationInterfaceCount && GSimStart != 1;
		bRunPhase0 = InstanceID < IterationInterfaceCount && GSimStart == 1;
	}
	#endif // USE_SHADER_STAGES

	const float RandomSeedInitialisation = NiagaraInternalNoise(InstanceID*16384, 0 * 8196, (bRunPhase1 ? 4096 : 0) + EmitterTickCounter);	// initialise the random state seed

	FSimulationContext Context = (FSimulationContext)0;
	InitConstants(Context);

	BRANCH
	if (bRunPhase1)
	{
		GCurrentPhase = 1;

		LoadUpdateVariables(Context, InstanceID);
		ReadDataSets(Context);

		SimulateMapUpdate(Context);
		WriteDataSets(Context);
	}
	else if (bRunPhase0)
	{
		GCurrentPhase = 0;

		GetEmitterSpawnInfoForParticle(ExecIndex());

		InitSpawnVariables(Context);
		ReadDataSets(Context);

		Context.MapSpawn.Particles.UniqueID = Engine_Emitter_TotalSpawnedParticles + ExecIndex();
		ConditionalInterpolateParameters(Context);
		SimulateMapSpawn(Context);

		GCurrentPhase = 1;

		TransferAttributes(Context);

		SimulateMapUpdate(Context);
		WriteDataSets(Context);
	}

	StoreUpdateVariables(Context);
}

#endif//GPU_SIMULATION
